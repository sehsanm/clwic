{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/common/en-678-xlmroberta.vec' , 'r' , encoding='utf8') as inp: \n",
    "  with open('../data/common/en-768-xlmroberta.vec' , 'w' , encoding='utf8') as out : \n",
    "    \n",
    "    out.write(inp.readline().strip()  + '\\n')\n",
    "    out.write(inp.readline().strip() + ' ' + inp.readline().strip() + '\\n') \n",
    "    l = inp.readline().strip()\n",
    "    while l: \n",
    "      out.write(l + '\\n') \n",
    "      l = inp.readline().strip() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Impossible to import Faiss library!! Switching to standard nearest neighbors search implementation, this will be significantly slower.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file  ../data/common/fa.txt\n",
      "264440  Tokens detected\n",
      "1000  Tokens Processed\n",
      "2000  Tokens Processed\n",
      "3000  Tokens Processed\n",
      "4000  Tokens Processed\n",
      "5000  Tokens Processed\n",
      "6000  Tokens Processed\n",
      "7000  Tokens Processed\n",
      "8000  Tokens Processed\n",
      "9000  Tokens Processed\n",
      "10000  Tokens Processed\n",
      "11000  Tokens Processed\n",
      "12000  Tokens Processed\n",
      "13000  Tokens Processed\n",
      "14000  Tokens Processed\n",
      "15000  Tokens Processed\n",
      "16000  Tokens Processed\n",
      "17000  Tokens Processed\n",
      "18000  Tokens Processed\n",
      "19000  Tokens Processed\n",
      "20000  Tokens Processed\n",
      "21000  Tokens Processed\n",
      "22000  Tokens Processed\n",
      "23000  Tokens Processed\n",
      "24000  Tokens Processed\n"
     ]
    }
   ],
   "source": [
    "import main as m\n",
    "from  transformers  import XLMRobertaModel, AutoTokenizer\n",
    "#print(pipeline('sentiment-analysis')('I was expected to be amazed but no'))\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# tokens = tokenizer.encode(\"Hello\") \n",
    "# print (tokenizer.decode(tokens))\n",
    "# input_ids = torch.tensor(tokens).unsqueeze(0)  # Batch size 1\n",
    "# print(input_ids.shape)\n",
    "# outputs = model(input_ids)\n",
    "# last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple    \n",
    "# print(last_hidden_states.shape)\n",
    "\n",
    "#build_embedding(model, tokenizer , 768 , 'data/common/en.txt' , 'data/common/en-678-xlmroberta.vec')\n",
    "m.build_embedding(model, tokenizer , 768 , '../data/common/fa.txt' , '../data/common/fa-768-xlmroberta.vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['264440', '768']\n"
     ]
    }
   ],
   "source": [
    "from evaluation.model import TextModel\n",
    "\n",
    "model =  TextModel('../data/common/fa-768-xlmroberta.vec' )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30072  Analogy entries loaded from  ../data/test/analogy_farsi.csv\n",
      "Running Analogy Test for : 30072\n",
      "Accuracy so far( 50 ): 0.02\n",
      "Accuracy so far( 100 ): 0.04\n",
      "Accuracy so far( 150 ): 0.02666666666666667\n",
      "Accuracy so far( 200 ): 0.02\n",
      "Accuracy so far( 250 ): 0.024\n",
      "Accuracy so far( 300 ): 0.02666666666666667\n",
      "Accuracy so far( 350 ): 0.025714285714285714\n"
     ]
    }
   ],
   "source": [
    "import evaluation.evaluation as ev\n",
    "\n",
    "analogy_data = ev.load_analogy('../data/test/analogy_farsi.csv')\n",
    "print(ev.run_analogy(model, analogy_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
