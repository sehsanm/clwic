{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded.\n"
     ]
    }
   ],
   "source": [
    "from  transformers  import XLMRobertaModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "print('Model and Tokenizer Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv \n",
    "\n",
    "def load_word_senses(input_file, word,   model , tokenizer): \n",
    "    dico = {}\n",
    "    with open(input_file, 'r' , encoding='utf8') as f: \n",
    "        csv_reader = csv.reader(f , delimiter='\\t')\n",
    "        for row in csv_reader:\n",
    "            if len(row) < 2:\n",
    "                print('Skipping Line:' , row)\n",
    "                continue\n",
    "                \n",
    "            tokens = tokenizer(row[1])\n",
    "            lst = tokenizer.convert_ids_to_tokens(tokens['input_ids']) \n",
    "\n",
    "            #Pass it to Model \n",
    "            input_ids = torch.tensor(tokens['input_ids']).unsqueeze(0)  \n",
    "            if input_ids.shape[0] > 500 or input_ids.shape[1] > 500:\n",
    "                print('Too long line - Skipping:')\n",
    "                continue \n",
    "                \n",
    "            outputs = model(input_ids)\n",
    "            #Build the Vectors\n",
    "            for ind , item in enumerate(lst): \n",
    "                if ind >= outputs[0].shape[1]:\n",
    "                    break\n",
    "                if item == '<s>' or item == '</s>':\n",
    "                    continue \n",
    "                else: \n",
    "                    item = item.replace('_' , '') \n",
    "                    if item == word: \n",
    "                        print ('({}) {}'.format(row[0], row[1]))\n",
    "                        if  row[0] not in dico:\n",
    "                            dico[row[0]] = []  \n",
    "                        dico[row[0]].append(outputs[0][0, ind , : ]) \n",
    "\n",
    "    return dico  \n",
    "\n",
    "milk_items= load_word_senses('../data/common/milk.txt', 'شیر',   model , tokenizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def  collect_vectors(corpus_file, words , model , tokenizer , max_found = 200): \n",
    "    dico = {}\n",
    "    with open(corpus_file, 'r' , encoding='utf8') as f: \n",
    "        found = 0 \n",
    "        for i, line in enumerate(f):\n",
    "            \n",
    "            if line.strip() == '' :\n",
    "                continue\n",
    "            l = line.strip().replace('.' , ' ')\n",
    "            in_list = False \n",
    "            added_line = ' ' + l + ' '\n",
    "            for w in words: \n",
    "                if ' ' + w + ' ' in added_line:\n",
    "                    in_list = True \n",
    "                    break \n",
    "            if not in_list:\n",
    "                continue \n",
    "                \n",
    "            tokens = tokenizer(l)\n",
    "            lst = tokenizer.convert_ids_to_tokens(tokens['input_ids']) \n",
    "            #do not pass it to model if none of the words appear in the list\n",
    "            in_list = False\n",
    "            for ind , item in enumerate(lst): \n",
    "                if item == '<s>' or item == '</s>':\n",
    "                    continue \n",
    "                else: \n",
    "                    item = item.replace('_' , '') \n",
    "                    if item in words: \n",
    "                        in_list = True \n",
    "                        break \n",
    "            if not in_list:\n",
    "                continue \n",
    "            #Pass it to Model \n",
    "            input_ids = torch.tensor(tokens['input_ids']).unsqueeze(0)  \n",
    "            if input_ids.shape[0] > 500 or input_ids.shape[1] > 500:\n",
    "                print('Too long line - Skipping:')\n",
    "                continue \n",
    "                \n",
    "            outputs = model(input_ids)\n",
    "            #Build the Vectors\n",
    "            for ind , item in enumerate(lst): \n",
    "                if ind >= outputs[0].shape[1]:\n",
    "                    break\n",
    "                if item == '<s>' or item == '</s>':\n",
    "                    continue \n",
    "                else: \n",
    "                    item = item.replace('_' , '') \n",
    "                    if item in words: \n",
    "                        found += 1\n",
    "                        print ('({}) {}'.format(found, l))\n",
    "                        if  item not in dico:\n",
    "                            dico[item] = []  \n",
    "                        dico[item].append(outputs[0][0, ind , : ]) \n",
    "            if found > max_found:\n",
    "                break\n",
    "    return dico  \n",
    "\n",
    "\n",
    "all_vectors = collect_vectors('../data/common/wiki.txt' , ['شیر'] , model, tokenizer)\n",
    "\n",
    "#print (tokenizer('یلام لبلنل لانمیب انیلب'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_in_2d(data_dictionary):\n",
    "    pca = PCA(n_components=2)\n",
    "    total_vectors = 0\n",
    "    dim = 0 \n",
    "\n",
    "    for key, value in data_dictionary.items(): \n",
    "        print('---' , key , ' ' , len(value))\n",
    "\n",
    "        total_vectors += len(value)\n",
    "        if len(value) > 0:\n",
    "            dim = value[0].shape[0]\n",
    "\n",
    "    x = np.zeros((total_vectors, dim))\n",
    "    rev_index = {} \n",
    "    ind = 0 \n",
    "    for key, value in data_dictionary.items(): \n",
    "        print('--' , key)\n",
    "        rev_index[key] = [] \n",
    "        for vec in value:\n",
    "            rev_index[key].append(ind) \n",
    "            print(key)\n",
    "            x[ind , :] = vec.detach().numpy() \n",
    "            ind += 1\n",
    "\n",
    "    x_2d = pca.fit_transform(x)\n",
    "    print(rev_index)\n",
    "    for key in data_dictionary: \n",
    "        plt.scatter(x_2d[ rev_index[key] , 0] , x_2d[ rev_index[key] , 1] , label=key , color=np.random.rand(3,))\n",
    "    plt.show()\n",
    "\n",
    "draw_in_2d(milk_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
