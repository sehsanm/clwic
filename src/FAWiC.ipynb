{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded.\n"
     ]
    }
   ],
   "source": [
    "from  transformers  import XLMRobertaModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "print('Model and Tokenizer Loaded.')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "from  collections import namedtuple\n",
    "from scipy.spatial import distance\n",
    "import re \n",
    "\n",
    "def match_tokens(normal_tokens , model_tokens):\n",
    "    nt_index = 0 \n",
    "    mt_index = 1 \n",
    "    ret = [] \n",
    "#     print ('Matching ' , normal_tokens, ' ' , model_tokens)\n",
    "    while nt_index < len(normal_tokens) and mt_index < len(model_tokens): \n",
    "#         print (model_tokens[mt_index] , '<->' , normal_tokens[nt_index])\n",
    "        if  model_tokens[mt_index].replace('▁' , '')  == normal_tokens[nt_index] :\n",
    "            ret.append((nt_index , mt_index)) \n",
    "            nt_index += 1 \n",
    "            mt_index += 1 \n",
    "        elif  normal_tokens[nt_index].startswith(model_tokens[mt_index].replace('▁' , '')) or  model_tokens[mt_index].replace('▁' , '').startswith(normal_tokens[nt_index]): \n",
    "            ret.append((nt_index , mt_index)) \n",
    "            nt_index += 1 \n",
    "            mt_index += 1 \n",
    "        else:\n",
    "            mt_index += 1 \n",
    "    return ret  \n",
    "\n",
    "def load_wic_data(file_name , is_train=True):\n",
    "    DataModel = namedtuple('DataModel' , 'word sent_a sent_b is_same')\n",
    "    ret = [] \n",
    "    with open(file_name,  'r' , encoding='utf8') as f: \n",
    "        data_file = csv.reader(f , delimiter='\\t') \n",
    "        for row in data_file:\n",
    "            if is_train: \n",
    "                ret.append(DataModel(row[0], row[6].replace('\\u200c' , ' ') , row[7].replace('\\u200c' , ' ') , row[8]))\n",
    "            else:\n",
    "                ret.append(DataModel(row[0], row[6].replace('\\u200c' , ' ') , row[7].replace('\\u200c' , ' ') , '0'))\n",
    "    return ret \n",
    "\n",
    "\n",
    "def classify(train, model , tokenizer , threshold): \n",
    "    success = 0 \n",
    "    for index,data in enumerate(train): \n",
    "        sent_a_tokens = data.sent_a.split(' ')\n",
    "        sent_b_tokens = data.sent_b.split(' ')\n",
    "        tokens_a = tokenizer(data.sent_a)\n",
    "        tokens_b = tokenizer(data.sent_b)\n",
    "        input_ids_a = torch.tensor(tokens_a['input_ids']).unsqueeze(0)  \n",
    "        input_ids_b = torch.tensor(tokens_b['input_ids']).unsqueeze(0)  \n",
    "        model_tokens_a = tokenizer.convert_ids_to_tokens(tokens_a['input_ids']) \n",
    "        model_tokens_b = tokenizer.convert_ids_to_tokens(tokens_b['input_ids']) \n",
    "\n",
    "        outputs_a = model(input_ids_a)        \n",
    "        outputs_b = model(input_ids_b)\n",
    "        \n",
    "        match_a = match_tokens(sent_a_tokens , model_tokens_a)\n",
    "        match_b = match_tokens(sent_b_tokens , model_tokens_b)\n",
    "        \n",
    "        \n",
    "#         print(data)\n",
    "#         print(model_tokens_a)\n",
    "#         print(sent_a_tokens)\n",
    "#         print(model_tokens_b)\n",
    "#         print(sent_b_tokens)\n",
    "        ind_a = 0 \n",
    "        ind_b = 0 \n",
    "        \n",
    "        for x,y in match_a: \n",
    "#             print('a' , sent_a_tokens[x])\n",
    "            if sent_a_tokens[x].startswith(data.word) or data.word.startswith(sent_a_tokens[x]) : \n",
    "                break \n",
    "            ind_a += 1 \n",
    "        \n",
    "        for x,y in match_b: \n",
    "#             print('b' , sent_b_tokens[x])\n",
    "            if sent_b_tokens[x].startswith(data.word) or data.word.startswith(sent_b_tokens[x]) : \n",
    "                break \n",
    "            ind_b += 1 \n",
    "\n",
    "    \n",
    "        if ind_a >= len(match_a) or ind_b >= len(match_b): \n",
    "            print('Cannot find match {}  in data {}'.format(data.word , data))\n",
    "            break\n",
    "            continue \n",
    "        \n",
    "        diff = distance.cosine(outputs_a[0][0, ind_a , : ].detach().numpy(),  outputs_b[0][0, ind_b , : ].detach().numpy())\n",
    "        predict = '0'\n",
    "        if diff < threshold: \n",
    "            predict = '1'\n",
    "        \n",
    "        if predict == data.is_same:\n",
    "            success += 1 \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    print('Success Rate {}'.format(success / len(train)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate 0.505\n"
     ]
    }
   ],
   "source": [
    "train = load_wic_data('../data/wic/fa/valid.txt') \n",
    "classify(train , model, tokenizer, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
