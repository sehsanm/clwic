{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1999995', '300']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import evaluation.model as md \n",
    "model_static_en = md.TextModel('/mnt/c/Projects/PHD/NLP/CLWiC/data/models/crawl-300d-2M.vec')\n",
    "#model_static_es = md.TextModel('/gdrive/MyDrive/MUSE/data/wiki.es.vec')\n",
    "model_static_fa = md.FastTextModel('/mnt/c/Projects/PHD/NLP/CLWiC/data/models/blogs_skipgram_300_3.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vectors\n",
      "longew  longew\n",
      "latns  latns\n",
      "#efefef  #efefef\n",
      "longd  longd\n",
      "longm  longm\n",
      "shortsummary  shortsummary\n",
      "linecolor  linecolor\n",
      "clubnat  clubnat\n",
      "nationalteam  nationalteam\n",
      "youthclubs  youthclubs\n",
      "youthyears  youthyears\n",
      "managerclubs  managerclubs\n",
      "nationalyears  nationalyears\n",
      "currentclub  currentclub\n",
      "manageryears  manageryears\n",
      "listclass  listclass\n",
      "irdt  irdt\n",
      "#aaa  #aaa\n",
      "wmflabs  wmflabs\n",
      "nationalcaps  nationalcaps\n",
      "nationalgoals  nationalgoals\n",
      "pcupdate  pcupdate\n",
      "Total 8869 processed, 8847 word found\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "from scipy.special import softmax\n",
    "\n",
    "import numpy as np\n",
    "import re \n",
    "\n",
    "class ProcrustesModel(md.Model):\n",
    "    def __init__(self, model , w):\n",
    "        self.model = model\n",
    "        self.w = w\n",
    "    def get_word_vector(self, word):\n",
    "        return np.matmul(self.w ,  self.model.get_word_vector(word).transpose() ).transpose() \n",
    "\n",
    "    def word_exist(self, word):\n",
    "        return self.model.word_exist(word)\n",
    "\n",
    "\n",
    "    def get_word_in_index(self, index):\n",
    "        return self.model.get_word_in_index(index)\n",
    "\n",
    "    def get_word_index(self, word):\n",
    "        return self.model.get_word_index(word)\n",
    "\n",
    "class LocalConverted(md.Model):\n",
    "    def __init__(self, src_model , support_vecs_src , support_vecs_dst):\n",
    "        self.model = src_model\n",
    "        self.support_vecs_src = support_vecs_src\n",
    "        self.support_vecs_dst = support_vecs_dst \n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        # print('B ' , word)\n",
    "        v =   batch_convert(self.support_vecs_src , self.support_vecs_dst , np.reshape(self.model.get_word_vector(word),(1 , -1) ))\n",
    "        return v.flatten() \n",
    "    def word_exist(self, word):\n",
    "        return self.model.word_exist(word)\n",
    "\n",
    "    def get_word_in_index(self, index):\n",
    "        return self.model.get_word_in_index(index)\n",
    "\n",
    "    def get_word_index(self, word):\n",
    "        return self.model.get_word_index(word)\n",
    "\n",
    "\n",
    "def  load_dictionary(src_tgt_file):\n",
    "  ret = [] \n",
    "  dic_map = {} \n",
    "  dic_rev_map = {} \n",
    "  with open(src_tgt_file, 'r' , encoding='utf8 ') as dict_file: \n",
    "    for ln , line in enumerate(dict_file):\n",
    "      ret.append( re.split('\\s|\\t' , line.strip()))\n",
    "      dic_map[ret[-1][0]] = ret[-1][1]\n",
    "  \n",
    "  dic_rev_map[ret[-1][1]] = ret[-1][0]\n",
    "  return ret , dic_map , dic_rev_map \n",
    "\n",
    "\n",
    "\n",
    "def find_k_neighbhor(support_vectors, query, k = 7): \n",
    "  dists = np.zeros(support_vectors.shape[0] )\n",
    "  for i,x in enumerate(support_vectors): \n",
    "    \n",
    "    dists[i] = distance.cosine(support_vectors[i, :] ,  query)\n",
    "\n",
    "  k_n = np.argsort(dists)[0:k]\n",
    "  k_n_d  = dists[k_n]\n",
    "  return k_n, k_n_d \n",
    "\n",
    "def load_vectors(src_model, tgt_model, dict):\n",
    "  print('Loading Vectors')\n",
    "  cnt = 0 \n",
    "  dim = 0 \n",
    "  for entry in dict: \n",
    "    if src_model.word_exist(entry[0]) and tgt_model.word_exist(entry[1]):\n",
    "      if cnt == 0 :\n",
    "        dim = src_model.get_word_vector(entry[0]).shape[0] \n",
    "      cnt += 1 \n",
    "    elif len(entry) == 2: \n",
    "      print('{}  {}'.format(entry[0] , entry[1]))\n",
    "    else:\n",
    "      print(entry)\n",
    "  src_vec = np.zeros((cnt, dim))\n",
    "  tgt_vec = np.zeros((cnt, dim))\n",
    "  cnt = 0 \n",
    "  for entry in dict: \n",
    "    if src_model.word_exist(entry[0]) and tgt_model.word_exist(entry[1]):\n",
    "      src_vec[cnt, :] = src_model.get_word_vector(entry[0]) \n",
    "      tgt_vec[cnt, :] = tgt_model.get_word_vector(entry[1])\n",
    "      cnt += 1\n",
    "\n",
    "  print('Total {} processed, {} word found'.format(len(dict), cnt))\n",
    "  return src_vec, tgt_vec \n",
    "\n",
    "\n",
    "def batch_convert(support_vectors_src, support_vectors_dst ,  queries ):\n",
    "  ret = np.zeros((queries.shape[0] , support_vectors_dst.shape[1]))\n",
    "  for i in range(queries.shape[0]): \n",
    "    k_n, k_n_d = find_k_neighbhor(support_vectors_src , queries[i , :]) \n",
    "    weights = softmax(k_n_d) \n",
    "    ret[i, :] = np.matmul(weights,support_vectors_dst[k_n , :])\n",
    "  \n",
    "  return ret \n",
    "\n",
    "def calculate_mean_dict_distance(src_vec, tgt_vec): \n",
    "  sum = 0\n",
    "  for ind in range(src_vec.shape[0]): \n",
    "      sum += distance.cosine(src_vec[ind, :], tgt_vec[ind, :]) \n",
    "  euclid_dist = np.linalg.norm(src_vec - tgt_vec)\n",
    "  return sum / src_vec.shape[0]  , euclid_dist / src_vec.shape[0] \n",
    "\n",
    "def procrustes(A, B):\n",
    "    \"\"\"\n",
    "    Find the best orthogonal matrix mapping using the Orthogonal Procrustes problem\n",
    "    https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem\n",
    "    \"\"\"\n",
    "    M = B.transpose().dot(A)\n",
    "    U, S, V_t = np.linalg.svd(M, full_matrices=True)\n",
    "    return  U.dot(V_t)\n",
    "\n",
    "lst, dic_map , dic_rev_map = load_dictionary('/mnt/c/Projects/PHD/NLP/CLWiC/data/dictionaries/en-fa.0-5000.txt')\n",
    "lst_full, dic_map_full , dic_rev_map_full = load_dictionary('/mnt/c/Projects/PHD/NLP/CLWiC/data/dictionaries/en-fa.txt')\n",
    "\n",
    "model_src = model_static_en\n",
    "model_dst = model_static_fa\n",
    "support_vecs_src, support_vecs_dst = load_vectors(model_src, model_dst ,  lst)  \n",
    "W = procrustes(support_vecs_dst , support_vecs_src) \n",
    "\n",
    "model_proc = ProcrustesModel(model_dst , W)\n",
    "model_local = LocalConverted(model_dst , support_vecs_dst , support_vecs_src) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score - Base!: (-0.0369644402901804, 702, 1) \n",
      "Similarity Score - Proc: (0.5450068416792695, 702, 1) \n",
      "Similarity Score - Local: (0.4255112135333219, 702, 1) \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io \n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "def get_word_pairs(path, lower=True):\n",
    "    \"\"\"\n",
    "    Return a list of (word1, word2, score) tuples from a word similarity file.\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(path) and type(lower) is bool\n",
    "    word_pairs = []\n",
    "    with io.open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            line = line.lower() if lower else line\n",
    "            line = line.split()\n",
    "            # ignore phrases, only consider words\n",
    "            if len(line) != 3:\n",
    "                assert len(line) > 3\n",
    "                assert 'SEMEVAL17' in os.path.basename(path) or 'EN-IT_MWS353' in path\n",
    "                continue\n",
    "            word_pairs.append((line[0], line[1], float(line[2])))\n",
    "    return word_pairs\n",
    "\n",
    "  \n",
    "\n",
    "def get_spearman_rho(model_src , model_dst, path):\n",
    "  \"\"\"\n",
    "  Compute monolingual or cross-lingual word similarity score.\n",
    "  \"\"\"\n",
    "  word_pairs = get_word_pairs(path)\n",
    "  not_found = 0\n",
    "  pred = []\n",
    "  gold = []\n",
    "  cnt = 0 \n",
    "  for word1, word2, similarity in word_pairs:\n",
    "      if not model_src.word_exist(word1) or not model_dst.word_exist(word2):\n",
    "          # if model_src.word_exist(word1):\n",
    "          #   print('Cannot find >{}< in dst {} '.format(word2 , model_dst.word_exist(word2) ))\n",
    "          # else:\n",
    "          #   print('Cannot find >{}< in src'.format(word1))\n",
    "\n",
    "          not_found += 1\n",
    "          continue\n",
    "      cnt += 1\n",
    "      u = model_src.get_word_vector(word1)\n",
    "      v = model_dst.get_word_vector(word2)\n",
    "      score = u.dot(v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "      # if cnt == 1 :\n",
    "      #   print('A ' , word1 , ' ' , word2 , ' ' , similarity, ' ' , score)\n",
    "      gold.append(similarity)\n",
    "      pred.append(score)\n",
    "  return spearmanr(gold, pred).correlation, len(gold), not_found\n",
    "print('Similarity Score - Base!: {} '.format(get_spearman_rho(model_src , model_dst , '../data/dictionaries/en-fa-SEMEVAL17.txt')))\n",
    "print('Similarity Score - Proc: {} '.format(get_spearman_rho(model_src , model_proc , '../data/dictionaries/en-fa-SEMEVAL17.txt')))\n",
    "print('Similarity Score - Local: {} '.format(get_spearman_rho(model_src , model_local , '../data/dictionaries/en-fa-SEMEVAL17.txt')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
