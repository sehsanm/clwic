{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['2519370', '300']\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7ba3a6c710be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_static_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/models/wiki.en.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#model_static_es = md.TextModel('/gdrive/MyDrive/MUSE/data/wiki.es.vec')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_static_fa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFastTextModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/mahmoudi/data/blogs_skipgram_300_3.out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/clwic/src/evaluation/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_name)\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# avoid to have null embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                         \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import evaluation.model as md \n",
    "model_static_en = md.TextModel('../data/models/wiki.en.vec')\n",
    "#model_static_es = md.TextModel('/gdrive/MyDrive/MUSE/data/wiki.es.vec')\n",
    "model_static_fa = md.FastTextModel('/home/mahmoudi/data/blogs_skipgram_300_3.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from scipy.special import softmax\n",
    "\n",
    "import numpy as np\n",
    "import re \n",
    "\n",
    "class ProcrustesModel(md.Model):\n",
    "    def __init__(self, model , w):\n",
    "        self.model = model\n",
    "        self.w = w\n",
    "    def get_word_vector(self, word):\n",
    "        return np.matmul(self.w ,  self.model.get_word_vector(word).transpose() ).transpose() \n",
    "\n",
    "    def word_exist(self, word):\n",
    "        return self.model.word_exist(word)\n",
    "\n",
    "\n",
    "    def get_word_in_index(self, index):\n",
    "        return self.model.get_word_in_index(index)\n",
    "\n",
    "    def get_word_index(self, word):\n",
    "        return self.model.get_word_index(word)\n",
    "\n",
    "class LocalConverted(md.Model):\n",
    "    def __init__(self, src_model , support_vecs_src , support_vecs_dst):\n",
    "        self.model = src_model\n",
    "        self.support_vecs_src = support_vecs_src\n",
    "        self.support_vecs_dst = support_vecs_dst \n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        # print('B ' , word)\n",
    "        v =   batch_convert(self.support_vecs_src , self.support_vecs_dst , np.reshape(self.model.get_word_vector(word),(1 , -1) ))\n",
    "        return v.flatten() \n",
    "    def word_exist(self, word):\n",
    "        return self.model.word_exist(word)\n",
    "\n",
    "    def get_word_in_index(self, index):\n",
    "        return self.model.get_word_in_index(index)\n",
    "\n",
    "    def get_word_index(self, word):\n",
    "        return self.model.get_word_index(word)\n",
    "\n",
    "\n",
    "def  load_dictionary(src_tgt_file):\n",
    "  ret = [] \n",
    "  dic_map = {} \n",
    "  dic_rev_map = {} \n",
    "  with open(src_tgt_file, 'r' , encoding='utf8 ') as dict_file: \n",
    "    for ln , line in enumerate(dict_file):\n",
    "      ret.append( re.split('\\s|\\t' , line.strip()))\n",
    "      dic_map[ret[-1][0]] = ret[-1][1]\n",
    "  \n",
    "  dic_rev_map[ret[-1][1]] = ret[-1][0]\n",
    "  return ret , dic_map , dic_rev_map \n",
    "\n",
    "\n",
    "\n",
    "def find_k_neighbhor(support_vectors, query, k = 5): \n",
    "  dists = np.zeros(support_vectors.shape[0] )\n",
    "  for i,x in enumerate(support_vectors): \n",
    "    \n",
    "    dists[i] = distance.cosine(support_vectors[i, :] ,  query)\n",
    "\n",
    "  k_n = np.argsort(dists)[0:k]\n",
    "  k_n_d  = dists[k_n]\n",
    "  return k_n, k_n_d \n",
    "\n",
    "def load_vectors(src_model, tgt_model, dict):\n",
    "  print('Loading Vectors')\n",
    "  cnt = 0 \n",
    "  dim = 0 \n",
    "  for entry in dict: \n",
    "    if src_model.word_exist(entry[0]) and tgt_model.word_exist(entry[1]):\n",
    "      if cnt == 0 :\n",
    "        dim = src_model.get_word_vector(entry[0]).shape[0] \n",
    "      cnt += 1 \n",
    "    elif len(entry) == 2: \n",
    "      print('{}  {}'.format(entry[0] , entry[1]))\n",
    "    else:\n",
    "      print(entry)\n",
    "  src_vec = np.zeros((cnt, dim))\n",
    "  tgt_vec = np.zeros((cnt, dim))\n",
    "  cnt = 0 \n",
    "  for entry in dict: \n",
    "    if src_model.word_exist(entry[0]) and tgt_model.word_exist(entry[1]):\n",
    "      src_vec[cnt, :] = src_model.get_word_vector(entry[0]) \n",
    "      tgt_vec[cnt, :] = tgt_model.get_word_vector(entry[1])\n",
    "      cnt += 1\n",
    "\n",
    "  print('Total {} processed, {} word found'.format(len(dict), cnt))\n",
    "  return src_vec, tgt_vec \n",
    "\n",
    "\n",
    "def batch_convert(support_vectors_src, support_vectors_dst ,  queries ):\n",
    "  ret = np.zeros((queries.shape[0] , support_vectors_dst.shape[1]))\n",
    "  for i in range(queries.shape[0]): \n",
    "    k_n, k_n_d = find_k_neighbhor(support_vectors_src , queries[i , :]) \n",
    "    weights = softmax(k_n_d) \n",
    "    ret[i, :] = np.matmul(weights,support_vectors_dst[k_n , :])\n",
    "  \n",
    "  return ret \n",
    "\n",
    "def calculate_mean_dict_distance(src_vec, tgt_vec): \n",
    "  sum = 0\n",
    "  for ind in range(src_vec.shape[0]): \n",
    "      sum += distance.cosine(src_vec[ind, :], tgt_vec[ind, :]) \n",
    "  euclid_dist = np.linalg.norm(src_vec - tgt_vec)\n",
    "  return sum / src_vec.shape[0]  , euclid_dist / src_vec.shape[0] \n",
    "\n",
    "def procrustes(A, B):\n",
    "    \"\"\"\n",
    "    Find the best orthogonal matrix mapping using the Orthogonal Procrustes problem\n",
    "    https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem\n",
    "    \"\"\"\n",
    "    M = B.transpose().dot(A)\n",
    "    U, S, V_t = np.linalg.svd(M, full_matrices=True)\n",
    "    return  U.dot(V_t)\n",
    "\n",
    "lst, dic_map , dic_rev_map = load_dictionary('../data/dictionaries/en-fa.0-5000.txt')\n",
    "lst_full, dic_map_full , dic_rev_map_full = load_dictionary('../data/crosslingual/dictionaries/en-fa.txt')\n",
    "\n",
    "model_src = model_static_en\n",
    "model_dst = model_static_fa\n",
    "support_vecs_src, support_vecs_dst = load_vectors(model_src, model_dst ,  lst)  \n",
    "W = procrustes(support_vecs_dst , support_vecs_src) \n",
    "\n",
    "model_proc = ProcrustesModel(model_dst , W)\n",
    "model_local = LocalConverted(model_dst , support_vecs_dst , support_vecs_src) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io \n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "def get_word_pairs(path, lower=True):\n",
    "    \"\"\"\n",
    "    Return a list of (word1, word2, score) tuples from a word similarity file.\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(path) and type(lower) is bool\n",
    "    word_pairs = []\n",
    "    with io.open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            line = line.lower() if lower else line\n",
    "            line = line.split()\n",
    "            # ignore phrases, only consider words\n",
    "            if len(line) != 3:\n",
    "                assert len(line) > 3\n",
    "                assert 'SEMEVAL17' in os.path.basename(path) or 'EN-IT_MWS353' in path\n",
    "                continue\n",
    "            word_pairs.append((line[0], line[1], float(line[2])))\n",
    "    return word_pairs\n",
    "\n",
    "  \n",
    "\n",
    "def get_spearman_rho(model_src , model_dst, path):\n",
    "  \"\"\"\n",
    "  Compute monolingual or cross-lingual word similarity score.\n",
    "  \"\"\"\n",
    "  word_pairs = get_word_pairs(path)\n",
    "  not_found = 0\n",
    "  pred = []\n",
    "  gold = []\n",
    "  cnt = 0 \n",
    "  for word1, word2, similarity in word_pairs:\n",
    "      if not model_src.word_exist(word1) or not model_dst.word_exist(word2):\n",
    "          # if model_src.word_exist(word1):\n",
    "          #   print('Cannot find >{}< in dst {} '.format(word2 , model_dst.word_exist(word2) ))\n",
    "          # else:\n",
    "          #   print('Cannot find >{}< in src'.format(word1))\n",
    "\n",
    "          not_found += 1\n",
    "          continue\n",
    "      cnt += 1\n",
    "      u = model_src.get_word_vector(word1)\n",
    "      v = model_dst.get_word_vector(word2)\n",
    "      score = u.dot(v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "      # if cnt == 1 :\n",
    "      #   print('A ' , word1 , ' ' , word2 , ' ' , similarity, ' ' , score)\n",
    "      gold.append(similarity)\n",
    "      pred.append(score)\n",
    "  return spearmanr(gold, pred).correlation, len(gold), not_found\n",
    "print('Similarity Score - Base!: {} '.format(get_spearman_rho(model_src , model_dst , '../data/dictionaries/en-fa-SEMEVAL17.txt')))\n",
    "print('Similarity Score - Proc: {} '.format(get_spearman_rho(model_src , model_proc , '../data/dictionaries/en-fa-SEMEVAL17.txt')))\n",
    "print('Similarity Score - Local: {} '.format(get_spearman_rho(model_src , model_local , '../data/dictionaries/en-fa-SEMEVAL17.txt')))\n"
   ]
  }
 ]
}